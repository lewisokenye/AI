  ## Responsible AI Detective Blog
## Case #1: The Hiring Bot That Hates Career Gaps

 What is happening:
A company uses AI to screen job applications. The system automatically lowers scores for candidates with “career gaps.”

 What is problematic:

 **Bias:** Women and caregivers in general are more likely to have gaps due to childcare or family duties. The AI ends up punishing them unfairly.
 **Transparency Issue:** Applicants never know that the “gap” rule killed their chance.

**One fix:**
Redesign the scoring system to contextualize career gaps instead of treating them as red flags, the AI could weigh skills and performance more heavily. In addition, give applicants a plain-language explanation of rejection reasons.

## Case #2: The Proctoring AI With Shifty Eyes

 What is happening:
Schools use AI proctoring software to watch students during online exams. The system flags “suspicious” behavior based on eye movement or fidgeting.

 What is problematic:

 **Discrimination:** Neurodivergent students, people with tics or even those with different cultural norms may be unfairly flagged.
 **Stress Factor:** Being watched and flagged incorrectly increases anxiety which can hurt performance and significance.

**One fix:**
Make the AI a support tool and not a judge. Use it to highlight potential cases for a human reviewer instead of auto-flagging students as cheaters. Add accessibility options for students with documented needs.

**Detective’s Closing Note:**
AI isn’t the villain, it is more like an overeager rookie cop. Without fairness checks and human oversight, it makes clumsy mistakes. Our job is to train it better, keep it accountable and make sure justice is served.

